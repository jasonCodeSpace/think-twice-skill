#!/usr/bin/env python3
"""
Think-Twice: Enhanced Startup Idea Analyzer
åŸºäºå¤±è´¥åˆ›ä¸šæ¡ˆä¾‹æ•°æ®åº“çš„æ‰¹åˆ¤æ€§åˆ†æå·¥å…·
"""

import json
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# ========================================================================
# é…ç½®åŒºåŸŸ - ç”¨æˆ·å¯ä»¥ä¿®æ”¹è¿™äº›è·¯å¾„
# ========================================================================

DEFAULT_DATA_DIR = Path(__file__).parent
CLASSIFIED_FILE = DEFAULT_DATA_DIR / "data" / "startups_classified.json"
VECTORS_FILE = DEFAULT_DATA_DIR / "data" / "startups_vectors.json"


# ========================================================================
# æ•°æ®åŠ è½½
# ========================================================================

def load_data(classified_path=CLASSIFIED_FILE, vectors_path=VECTORS_FILE):
    """åŠ è½½åˆ›ä¸šå…¬å¸æ•°æ®å’Œå‘é‡æ•°æ®"""
    with open(classified_path, 'r', encoding='utf-8') as f:
        startups = json.load(f)

    with open(vectors_path, 'r', encoding='utf-8') as f:
        vectors_data = json.load(f)

    return startups, vectors_data


# å…¨å±€åŠ è½½ï¼ˆæ‡’åŠ è½½ï¼‰
_startups = None
_vectors_data = None
_id_to_startup = None
_id_to_vector = None
_all_ids = None
_embedding_matrix = None
_embed_model = None


def init():
    """åˆå§‹åŒ–åˆ†æå™¨"""
    global _startups, _vectors_data, _id_to_startup, _id_to_vector, _all_ids, _embedding_matrix, _embed_model

    if _startups is None:
        _startups, _vectors_data = load_data()
        _id_to_startup = {s['id']: s for s in _startups}
        _id_to_vector = {v['id']: v['embedding'] for v in _vectors_data}
        _all_ids = sorted(_id_to_vector.keys())
        _embedding_matrix = np.array([_id_to_vector[id] for id in _all_ids])
        _embed_model = SentenceTransformer('all-MiniLM-L6-v2')

    return _startups, _vectors_data


# ========================================================================
# åˆ†æå‡½æ•°
# ========================================================================

def analyze_failure_path(startup):
    """æ·±åº¦åˆ†æä¸€ä¸ªå¤±è´¥æ¡ˆä¾‹çš„å®Œæ•´è·¯å¾„"""
    desc = startup.get('description', '')

    problem = ""
    solution = ""
    desc_lower = desc.lower()

    # æ™ºèƒ½æå–é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
    for marker in ['solve the problem', 'address the problem', 'problem of',
                    'aimed to', 'sought to', 'designed to']:
        if marker in desc_lower:
            idx = desc_lower.find(marker)
            problem = desc[:idx].strip()
            rest = desc[idx + len(marker):].strip()
            solution = (marker + " " + rest.split('.')[0]).strip()
            break

    if not problem:
        problem = desc[:300]

    failure_mode = startup.get('difficulty_reason', '')
    scalability_issue = startup.get('scalability_reason', '')

    # ç°çŠ¶æ¨æ–­
    end_year = startup.get('end_year', 0)
    funding = startup.get('total_funding', 0)
    if end_year and end_year < 2025:
        if funding > 1_000_000:
            aftermath = f"å·²äº{end_year}å¹´å€’é—­ï¼Œçƒ§æ‰${funding/1_000_000:.1f}Mèèµ„åæ— ç–¾è€Œç»ˆ"
        else:
            aftermath = f"å·²äº{end_year}å¹´å€’é—­ï¼Œèèµ„è¾ƒå°‘ï¼Œæœªèƒ½éªŒè¯å•†ä¸šæ¨¡å¼"
    else:
        aftermath = "å·²åœæ­¢è¿è¥"

    return {
        'problem': problem[:300],
        'solution': solution[:300] if solution else "è¯¦è§å®Œæ•´æè¿°",
        'failure_mode': failure_mode[:400],
        'scalability_issue': scalability_issue[:400],
        'aftermath': aftermath
    }


def comprehensive_analysis(idea: str, top_k: int = 10):
    """ç»¼åˆåˆ†æåˆ›ä¸šæƒ³æ³•"""
    init()  # ç¡®ä¿æ•°æ®å·²åŠ è½½

    # å‘é‡æœç´¢
    query_embedding = _embed_model.encode(idea).reshape(1, -1)
    similarities = cosine_similarity(query_embedding, _embedding_matrix)[0]
    top_indices = np.argsort(similarities)[::-1][:top_k]

    results = []
    for idx in top_indices:
        startup_id = _all_ids[idx]
        startup = _id_to_startup.get(startup_id)
        if not startup:
            continue

        failure_path = analyze_failure_path(startup)

        results.append({
            'similarity': float(similarities[idx]),
            'startup': startup,
            'failure_path': failure_path
        })

    # ç»Ÿè®¡åˆ†æ
    difficulties = [r['startup'].get('difficulty', 0) for r in results]
    scalabilities = [r['startup'].get('scalability', 0) for r in results]
    fundings = [r['startup'].get('total_funding', 0) for r in results]

    avg_difficulty = np.mean(difficulties)
    avg_scalability = np.mean(scalabilities)
    avg_funding = np.mean(fundings)

    # è®¡ç®—è¯„åˆ†
    difficulty_penalty = (avg_difficulty / 4) * 40
    scalability_penalty = ((4 - avg_scalability) / 4) * 30
    funding_pressure = min(avg_funding / 50_000_000 * 20, 20)
    feasibility_score = max(0, 100 - difficulty_penalty - scalability_penalty - funding_pressure)

    # è¡Œä¸šåˆ†å¸ƒ
    industries = {}
    for r in results:
        ind = r['startup'].get('primary_industry', 'Unknown')
        industries[ind] = industries.get(ind, 0) + 1

    main_industry = max(industries, key=industries.get) if industries else None

    return {
        'results': results,
        'score': feasibility_score,
        'stats': {
            'avg_difficulty': avg_difficulty,
            'avg_scalability': avg_scalability,
            'avg_funding': avg_funding,
            'difficulty_penalty': difficulty_penalty,
            'scalability_penalty': scalability_penalty,
            'funding_pressure': funding_pressure
        },
        'fundings': sorted(fundings),
        'industries': industries,
        'main_industry': main_industry
    }


# ========================================================================
# å‘½ä»¤è¡Œæ¥å£
# ========================================================================

if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python enhanced_analyzer.py '<your startup idea in English>'")
        print("\nExample:")
        print('  python enhanced_analyzer.py "Job search platform for MNC jobs in China"')
        sys.exit(1)

    idea = sys.argv[1]

    print("ğŸ” Analyzing startup idea...")
    analysis = comprehensive_analysis(idea, top_k=10)

    print(f"\n## ğŸ“Š å¯è¡Œæ€§è¯„åˆ†: {analysis['score']:.0f}/100")
    print(f"  - éš¾åº¦: {analysis['stats']['avg_difficulty']:.1f}/4 â†’ -{analysis['stats']['difficulty_penalty']:.0f}åˆ†")
    print(f"  - å¯æ‰©å±•æ€§: {analysis['stats']['avg_scalability']:.1f}/4 â†’ -{analysis['stats']['scalability_penalty']:.0f}åˆ†")
    print(f"  - èèµ„å‹åŠ›: ${analysis['stats']['avg_funding']:,.0f} â†’ -{analysis['stats']['funding_pressure']:.0f}åˆ†")

    n = len(analysis['fundings'])
    print(f"\n## ğŸ’° èèµ„å»ºè®®ï¼ˆåŸºäº{len(analysis['results'])}ä¸ªç›¸ä¼¼æ¡ˆä¾‹ï¼‰")
    print(f"  - 25åˆ†ä½: ${analysis['fundings'][n//4]:,.0f}")
    print(f"  - ä¸­ä½æ•°: ${analysis['fundings'][n//2]:,.0f}")
    print(f"  - 75åˆ†ä½: ${analysis['fundings'][n*3//4]:,.0f}")

    print(f"\n## ğŸ­ ä¸»è¦è¡Œä¸š: {analysis['main_industry']}")

    print(f"\n## âš ï¸ ç›¸ä¼¼å¤±è´¥æ¡ˆä¾‹è¯¦ç»†åˆ†æ")
    for i, r in enumerate(analysis['results'][:5], 1):
        s = r['startup']
        fp = r['failure_path']

        print(f"\n### {i}. {s['name']} ({r['similarity']:.1%})")
        print(f"**è¡Œä¸š**: {s.get('primary_industry', 'N/A')} | **èèµ„**: ${s.get('total_funding', 0):,} | **å¤±è´¥**: {s.get('end_year', 'N/A')}")
        print(f"\n**ğŸ¯ è¦è§£å†³çš„é—®é¢˜**:")
        print(f"  {fp['problem']}")
        print(f"\n**ğŸ’¡ è§£å†³æ–¹æ¡ˆ**:")
        print(f"  {fp['solution']}")
        print(f"\n**âŒ å¤±è´¥åŸå› **:")
        print(f"  {fp['failure_mode'][:200]}...")
        print(f"\n**ğŸ“ˆ å¯æ‰©å±•æ€§é—®é¢˜**:")
        print(f"  {fp['scalability_issue'][:200]}...")
        print(f"\n**ğŸ’€ ç°çŠ¶**:")
        print(f"  {fp['aftermath']}")
